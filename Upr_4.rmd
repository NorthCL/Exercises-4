---
title: "Упражнение № 4"
author: "Ямпольский Антон"
date: "23 03 2021"
output: html_document
---

### Вариант № 29 (11)

```{r setup, include=FALSE}

# загрузка пакетов
library('ISLR')         # загружаем пакет
library('GGally')       # графики совместного разброса переменных
library('lmtest')       # тесты остатков регрессионных моделей
library('FNN')          # алгоритм kNN

knitr::opts_chunk$set(echo = TRUE)

```


Цель: исследовать набор данных Carseats {ISLR} с помощью линейной регрессионной модели. Задействовав все возможные регрессоры, сделать вывод о пригодности модели для прогноза. Сравнить с методом k ближайших соседей по MSE на тестовой выборке.

#### Описание переменных

Набор данных Carseats содержит переменные:

*Sales* - Удельные продажи (в тысячах) в каждом месте;

*Price* - Цена, которую компания взимает за автокресла на каждом участке;

*Advertising* - Местный рекламный бюджет для компании в каждом месте (в тысячах долларов);

*ShelveLoc* - Фактор с уровнями Плохой, Хороший и Средний, указывающий на качество расположения стеллажей для автокресел на каждом участке;


```{r, echo=FALSE}

# константы
my.seed <- 29
train.percent <- 0.85

# открываем данные
data(Carseats)            
#?Carseats 

Carseats <- subset(Carseats, select = c(Sales, Price, Advertising, ShelveLoc))
#Carseats 

Carseats$ShelveLoc <- as.factor(Carseats$ShelveLoc)

str(Carseats) 

# обучающая выборка
set.seed(my.seed)
inTrain <- sample(seq_along(Carseats$Sales), 
                  nrow(Carseats) * train.percent)
df.train <- Carseats[inTrain, c(colnames(Carseats)[-1], colnames(Carseats)[1])]
df.test <- Carseats[-inTrain, -1]

```


Размерность обучающей выборки: $n = 400$ строк, $p = 3$ объясняющих переменных. Зависимая переменная – *Sales*. Дискретная величина - *ShelveLoc*.


## Oписательные статистики по переменным
```{r, echo=FALSE}

summary(df.train)

```


## Cовместный график разброса переменных.

Разобьем его на два графика, чтобы лучше понимать ситуацию

```{r, echo=FALSE, warning=FALSE, error = F}

ggp <- ggpairs(df.train[ , c(1, 3, 4)], upper = list(combo = 'box'))
print(ggp, progress = F)

ggp <- ggpairs(df.train[ , c(2, 3, 4)], upper = list(combo = 'box'))
print(ggp, progress = F)

```


```{r, echo=FALSE, warning=FALSE, error = FALSE}
# цвета по фактору ShelveLoc
ggpairs(df.train[, c('ShelveLoc', 'Sales', 'Price', 'Advertising')],
aes(color = ShelveLoc), upper = list(combo = 'box'))

```


Коробчатые диаграммы на пересечении *Sales* и *ShelveLoc* показывают, что удельные продажи (в тысячах) больше при хорошем уровнем расположения стеллажей. На остальные факторы (*Price* и *Advertising*)  фактор *ShelveLoc* влияния не оказывает. Так же можно заметить, что наблюдения распределены по значениям переменой *ShelveLoc* неравномерно: группа со средним уровнем расположения стеллажей самая многочисленная, а с плохим и хорошим уровнем практически одинаковы.


## Модели

```{r echo = F, warning = F, error = F}

model.1 <- lm(Sales ~ . + Price:ShelveLoc + ShelveLoc:Advertising,
              data = df.train)
summary(model.1)

```
В полученной модели есть незначимые парамеиры. Попробуем улучшить модель, исключив незначимые переменные. Первым исключаем совместное влияние *Price:ShelveLoc* исключаем, т.к. значение параметра незначимого параметра самое большое. 



```{r echo = F, warning = F, error = F}

model.2 <- lm(Sales ~ . + ShelveLoc:Advertising,
              data = df.train)
summary(model.2)

```


Теперь исключаем параметр *Advertising:ShelveLoc* так как он не значим


```{r echo = F, warning = F, error = F}

model.3 <- lm(Sales ~ .,
              data = df.train)
summary(model.3)

```

Мы получили значимую модель.

Попробуем сделать *ShelveLoc* дискретной количественной переменной


```{r echo = F, warning = F, error = F}
df.train$ShelveLoc <- as.numeric(df.train$ShelveLoc)
df.test$ShelveLoc <- as.numeric(df.test$ShelveLoc)

model.4 <- lm( Sales ~ Price + ShelveLoc + ShelveLoc:Price,
              data = df.train)
summary(model.4)
```

Значимость модели уменьшилась,  модель улучшить не получилось. Для дальнейшего исследования будем использовать модель 2. 

# Проверка остатков

```{r echo = F, warning = F, error = F}
# тест Бройша-Пагана
bptest(model.3)

# статистика Дарбина-Уотсона
dwtest(model.3)

# графики остатков
par(mar = c(4.5, 4.5, 2, 1))
par(mfrow = c(1, 3))

# график 1
plot(model.3, 1)

# график 2
plot(model.3, 4)

# график 3
plot(model.3, 5) 

par(mfrow = c(1, 1))

```
Судя по графику слева, остатки распредлены  не равномерно, и их дисперсия непостоянна. В модели есть три влиятельных наблюдения: 51, 377, 311, – которые, однако, не выходят за пределы доверительных границ на третьем графике. Графики остатков заставляют усомниться в том, что остатки удовлетворяют условиям Гаусса-Маркова.


# Сравнение с kNN

```{r echo = F}
# линейная модель
# фактические значения y на тестовой выборке
y.fact <- Carseats[-inTrain, ]$Sales
y.model.lm <- predict(model.4, df.test)
MSE.lm <- sum((y.model.lm - y.fact)^2) / length(y.model.lm)


# kNN требует на вход только числовые переменные
df.train.num <- as.data.frame(apply(df.train, 2, as.numeric))
df.test.num <- as.data.frame(apply(df.test, 2, as.numeric))

# цикл по k
for (i in 2:50){
model.knn <- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% 'Sales')], 
                     y = df.train.num[, 'Sales'], 
                     test = df.test.num, k = i)
y.model.knn <-  model.knn$pred
    if (i == 2){
        MSE.knn <- sum((y.model.knn - y.fact)^2) / length(y.model.knn)
    } else {
        MSE.knn <- c(MSE.knn, 
                     sum((y.model.knn - y.fact)^2) / length(y.model.knn))
    }
}

# график
par(mar = c(4.5, 4.5, 1, 1))
# ошибки kNN
plot(2:50, MSE.knn, type = 'b', col = 'darkgreen',
     xlab = 'значение k', ylab = 'MSE на тестовой выборке')
# ошибка регрессии
lines(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2)
legend('topright', lty = c(1, 2), pch = c(1, NA), 
       col = c('darkgreen', grey(0.2)), 
       legend = c('k ближайших соседа', 'регрессия (все факторы)'), 
       lwd = rep(2, 2))

```

 
```{r, echo = FALSE}
mean(Carseats$Sales)
```



Как можно видеть по графику, ошибка регрессии на тестовой выборке больше, чем ошибка метода k ближайших соседей с k от 2 до 30. Далее с увеличением количества соседей точность kNN падает. Ошибка регрессионной модели на тестовой выборке не велика и составляет

$$\frac{\sqrt{MSE_{TEST}}}{\bar{y}_{TEST}} = 20.8 % $$ 

от среднего значения зависимой переменной. Для модели регрессии это может означать присутствие всех важных оюъясняющих факторов.






